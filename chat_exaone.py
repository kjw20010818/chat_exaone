import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

# ëª¨ë¸ ê²½ë¡œ
model_id = "LGAI-EXAONE/EXAONE-4.0-32B"

print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
tokenizer = AutoTokenizer.from_pretrained(model_id)

# ì–‘ìí™” ì„¤ì • (8bit â†’ 4bitë¡œë„ ë³€ê²½ ê°€ëŠ¥)
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,      # VRAM ì—¬ìœ  ì—†ìœ¼ë©´ load_in_4bit=True ë¡œ ë³€ê²½
    llm_int8_threshold=6.0
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,   # A100ì´ë‹ˆ fp16 ê¶Œì¥
    quantization_config=bnb_config
)

# ëŒ€í™” ê¸°ë¡ ì €ì¥
chat_history = []

def chat_fn(message, history):
    global chat_history
    chat_history.append({"role": "user", "content": message})

    # chat_template ì ìš©
    prompt = tokenizer.apply_chat_template(
        chat_history,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=512,   # ë‹µë³€ ê¸¸ì´ ëŠ˜ë¦¼
        do_sample=True,
        temperature=0.7,
        top_p=0.9
    )

    # ìƒˆë¡œ ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    reply = tokenizer.decode(
        outputs[0][inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()

    chat_history.append({"role": "assistant", "content": reply})
    return reply

demo = gr.ChatInterface(fn=chat_fn)
demo.launch(server_name="0.0.0.0", server_port=7860)
